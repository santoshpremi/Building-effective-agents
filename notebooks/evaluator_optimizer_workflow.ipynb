{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215fe8e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate(prompt: str, task: str, context: str = \"\") -> tuple[str, str]:\n",
    "    \"\"\"Generate and improve a solution based on feedback.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\n{context}\\nTask: {task}\" if context else f\"{prompt}\\nTask: {task}\"\n",
    "    response = llm_call(full_prompt)\n",
    "    thoughts = extract_xml(response, \"thoughts\")\n",
    "    result = extract_xml(response, \"response\")\n",
    "    \n",
    "    print(\"\\n=== GENERATION START ===\")\n",
    "    print(f\"Thoughts:\\n{thoughts}\\n\")\n",
    "    print(f\"Generated:\\n{result}\")\n",
    "    print(\"=== GENERATION END ===\\n\")\n",
    "    \n",
    "    return thoughts, result\n",
    "\n",
    "def evaluate(prompt: str, content: str, task: str) -> tuple[str, str]:\n",
    "    \"\"\"Evaluate if a solution meets requirements.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\nOriginal task: {task}\\nContent to evaluate: {content}\"\n",
    "    response = llm_call(full_prompt)\n",
    "    evaluation = extract_xml(response, \"evaluation\")\n",
    "    feedback = extract_xml(response, \"feedback\")\n",
    "    \n",
    "    print(\"=== EVALUATION START ===\")\n",
    "    print(f\"Status: {evaluation}\")\n",
    "    print(f\"Feedback: {feedback}\")\n",
    "    print(\"=== EVALUATION END ===\\n\")\n",
    "    \n",
    "    return evaluation, feedback\n",
    "\n",
    "def loop(task: str, evaluator_prompt: str, generator_prompt: str) -> tuple[str, list[dict]]:\n",
    "    \"\"\"Keep generating and evaluating until requirements are met.\"\"\"\n",
    "    memory = []\n",
    "    chain_of_thought = []\n",
    "    \n",
    "    thoughts, result = generate(generator_prompt, task)\n",
    "    memory.append(result)\n",
    "    chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})\n",
    "    \n",
    "    while True:\n",
    "        evaluation, feedback = evaluate(evaluator_prompt, result, task)\n",
    "        if evaluation == \"PASS\":\n",
    "            return result, chain_of_thought\n",
    "            \n",
    "        context = \"\\n\".join([\n",
    "            \"Previous attempts:\",\n",
    "            *[f\"- {m}\" for m in memory],\n",
    "            f\"\\nFeedback: {feedback}\"\n",
    "        ])\n",
    "        \n",
    "        thoughts, result = generate(generator_prompt, task, context)\n",
    "        memory.append(result)\n",
    "        chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})\n",
    "\n",
    "evaluator_prompt = \"\"\"\n",
    "Evaluate this following code implementation for:\n",
    "1. code correctness\n",
    "2. time complexity\n",
    "3. style and best practices\n",
    "\n",
    "You should be evaluating only and not attemping to solve the task.\n",
    "Only output \"PASS\" if all criteria are met and you have no further suggestions for improvements.\n",
    "Output your evaluation concisely in the following format.\n",
    "\n",
    "<evaluation>PASS, NEEDS_IMPROVEMENT, or FAIL</evaluation>\n",
    "<feedback>\n",
    "What needs improvement and why.\n",
    "</feedback>\n",
    "\"\"\"\n",
    "\n",
    "generator_prompt = \"\"\"\n",
    "Your goal is to complete the task based on <user input>. If there are feedback \n",
    "from your previous generations, you should reflect on them to improve your solution\n",
    "\n",
    "Output your answer concisely in the following format: \n",
    "\n",
    "<thoughts>\n",
    "[Your understanding of the task and feedback and how you plan to improve]\n",
    "</thoughts>\n",
    "\n",
    "<response>\n",
    "[Your code implementation here]\n",
    "</response>\n",
    "\"\"\"\n",
    "\n",
    "task = \"\"\"\n",
    "<user input>\n",
    "Implement a Stack with:\n",
    "1. push(x)\n",
    "2. pop()\n",
    "3. getMin()\n",
    "All operations should be O(1).\n",
    "</user input>\n",
    "\"\"\"\n",
    "\n",
    "loop(task, evaluator_prompt, generator_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67535e8c",
   "metadata": {},
   "source": [
    "=== GENERATION START ===\n",
    "Thoughts:\n",
    "\n",
    "To implement a stack with O(1) operations for push, pop, and getMin, we need to track the minimum element efficiently. A common approach is to use an auxiliary stack (min_stack) that keeps track of the current minimum. \n",
    "\n",
    "- When pushing an element, we compare it with the current minimum (top of min_stack). If it's smaller or equal, we push it to min_stack.\n",
    "- When popping an element, if it's the current minimum (top of min_stack), we pop from min_stack as well.\n",
    "- getMin simply returns the top of min_stack.\n",
    "\n",
    "This ensures all operations are O(1) as required.\n",
    "\n",
    "\n",
    "Generated:\n",
    "\n",
    "class MinStack:\n",
    "    def __init__(self):\n",
    "        self.stack = []\n",
    "        self.min_stack = []\n",
    "\n",
    "    def push(self, x):\n",
    "        self.stack.append(x)\n",
    "        if not self.min_stack or x <= self.min_stack[-1]:\n",
    "            self.min_stack.append(x)\n",
    "\n",
    "    def pop(self):\n",
    "        if not self.stack:\n",
    "            return None\n",
    "        x = self.stack.pop()\n",
    "        if x == self.min_stack[-1]:\n",
    "            self.min_stack.pop()\n",
    "        return x\n",
    "\n",
    "    def getMin(self):\n",
    "        if not self.min_stack:\n",
    "            return None\n",
    "        return self.min_stack[-1]\n",
    "\n",
    "=== GENERATION END ==="
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
